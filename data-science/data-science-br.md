---
layout: default
title: Data Science PT-BR
description: Quest√µes de DataScience e Machine Learning
---

<!--
Conceitos
Supervisioned Learning
Avalia√ß√¢o de ML
Classifica√ß√¢o 
REgress√£o
Sle√ß√¢o de Features
√Årvore de Decis√£o
Gradient Boosting
Parameter tunings
Redu√ß√£o de dimensionalidade

O √≠ndex deve ser bem elaborado, deixando cada coisa junta:
usar h3 e h4, mas nao h5 pois √© pequeno demais

O que est√° separado aqui
+ Conceituais e Te√≥ricas
+ T√©cnicas
+ Avaliar Modelos
+Engenhraia de Features
-->

## √çndex

## Links
+ [https://br.bitdegree.org/tutoriais/data-science/#Dicas_gerais_e_resumo](https://br.bitdegree.org/tutoriais/data-science/#Dicas_gerais_e_resumo)

## Conceituais e Te√≥ricas

### DS-001 - O que √© Data Science?

Ci√™ncia de dados √© a atividade de extrair informa√ß√£o a apartir de dados, estruturados (usando ml) ou n√£o estruturados ( usando dl).

Data Science N√ÉO √â IA, Estat√≠stica, ML (Machine Learning), Big Data, Power BI nem Algoritmos.

### DS-002 - O que √© Big Data?

Big data significa ter enorme volume de dados. O termo em si significa isso mas pode engloba toda a parte de arquitetura de uma empresa para suportar ter esse dados (como o Hadoop).

### DS-003 - Qual √© a diferen√ßa entre ‚Äòdata science‚Äô e ‚Äòbig data‚Äô?

Big Data em si n√£o traz valor algum sem a t√©cnica de Data Science. Ent√£o, Big Data √© um objeto de an√°lise de Data Science.

### DS-004 - Qual √© a diferen√ßa entre um ‚Äòdata scientist‚Äô e um ‚Äòdata analyst‚Äô?

A DataScience busca retirar informa√ß√µes apartir de t√©cnicas computacionais: para Classifica√ß√£o, Regress√£o e etc..

Data Analytics resolve problemas de neg√≥cios, utilizando mais a estat√≠stica para resolver as coisas.

### DS-005 - Quais s√£o os recursos fundamentais que representam big data?

Agora que abordamos as defini√ß√µes, podemos passar para as perguntas mais espec√≠ficas de uma entrevista sobre data science. Tenha em mente, por√©m, que voc√™ ser√° obrigado a responder perguntas relacionadas a data scientist, data analyst e big data. A raz√£o para isso acontecer √© porque todas essas subcategorias est√£o interligadas entre si.

Existem cinco categorias que representam big data e s√£o chamadas de ‚Äù 5 Vs ‚Äú:

+ Valor;
+ Variedade;
+ Velocidade;
+ Veracidade;
+ Volume.

Todos esses termos correspondem ao Big Data de uma maneira ou de outra.

### DS-006 - Qual a diferen√ßa entre IA, ML e DL?

ML, DL e NN (Neural Networks) s√£o subconjuntos da √°rea de INtelig√™ncia Artificial

IA < ML < DL 

<img src="../img/difference-ia-ds-ml.png" />

ML √© uma √°rea de IA que trata de algoritmos ou t√©cnicas computacionais para m√°quinas/modelo/algoritmo aprender automaticamente com os dados.

DL √© um conjunto de ML que trata das redes neurais com v√°rias camadas e mais complexas.

---
---
---

## + T√©cnicas 

### DS-007 - O que √© Overfitting, Underfitting e Generalization, Bias e Vari√¢ncia?

<img src="../img/overffiting-underfiting.png" />

<img src="../img/bias-variance.png" />

+ **High Variance:** Baixo erro em dados de treino e alto erro em dados de teste
+ **High Bias:** Alto erro em dados de treino e erro parecido em dados de teste
+ **High Bias and Variance**: Alto erro em dados de treino e erro maior em dados de teste
+ **Low Bias and Variance:** Baixo erro em dados de treino e baixo erro em dados de teste

**What is low/high bias/variance?**

These concepts are important to understand k-Fold Cross Validation:
Low Bias is when your model predictions are very close to the real values.
High Bias is when your model predictions are far from the real values.
Low Variance: when you run your model several times, the different predictions of your observation points
won‚Äôt vary much.
High Variance: when you run your model several times, the different predictions of your observation points
will vary a lot.


### DS-010 - Qual √© a diferen√ßa entre o aprendizado ‚Äòsupervisionado‚Äô e ‚Äòn√£o supervisionado‚Äô?

Embora essa n√£o seja uma das perguntas mais comuns das entrevistas, e, tenha mais a ver com machine learning do que com qualquer outra coisa, ela ainda assim pertence ao data science, portanto vale a pena saber a resposta.

Durante o aprendizado supervisionado, voc√™ infere uma fun√ß√£o de uma parte rotulada de dados projetada para treinamento. Basicamente, a m√°quina aprenderia com os exemplos objetivos e concretos que voc√™ fornece.

Aprendizado n√£o supervisionado refere-se a um m√©todo de treinamento de m√°quina que n√£o usa respostas rotuladas ‚Äì a m√°quina aprende por descri√ß√µes dos dados de entrada.

### DS-012 - Qual a diferen√ßa entre Classifica√ß√£o e Regress√£o?

Os dois s√£o atividades que podem ser realizadas por modelos de ML para predi√ß√£o.

A principal diferen√ßa √© no valor de sa√≠da
+ Na classifica√ß√£o o valor √© discreto (0 ou 1, ou [0,1,2,3..] ...)
+ Na regress√£o √© cont√≠nuo (1,88; R$ 6.880,99 ...)

Em classifica√ß√£o, o objetivo √© classificar uma row em determinada categoria.
+ Exemplo: Dado uma row com caracter√≠sticas de uma flor, classificar que tipo de planta √© ela (Iris)

Em regress√£o, o objetivo √© obter um valor num√©rico
+ Exemplo: Dado uma row de caracter√≠sticas de uma casa, predizer o valor dela  


### DS-001 Qual a diferen√ßa entre covari√¢ncia e correla√ß√£o?

Correla√ß√£o √© a forma padronizada de covari√¢ncia.

Covari√¢ncias s√£o dif√≠ceis de comparar. Por exemplo: se calcularmos as covari√¢ncias de sal√°rio ($) e idade (anos), teremos diferentes
covari√¢ncias que n√£o podem ser comparadas por causa de escalas desiguais.

Para combater essa situa√ß√£o, calculamos a correla√ß√£o para obter um valor
entre -1 e 1, independentemente da respectiva escala.
 

### DS-001 - √â poss√≠vel capturar a correla√ß√£o entre vari√°vel cont√≠nua e vari√°vel categ√≥rica? Se sim, como?

Sim, podemos usar a t√©cnica ANCOVA (an√°lise de covari√¢ncia) para capturar a associa√ß√£o entre vari√°veis cont√≠nuas e categ√≥ricas.

O acr√¥nimo ANCOVA vem de ‚ÄúANalysis of COVAriance‚Äù. Na realidade, a ANCOVA combina dois tipos de estrat√©gias: An√°lise de Vari√¢ncia (ANOVA) e An√°lise de Regress√£o.

A an√°lise de covari√¢ncia permite aumentar a precis√£o dos experimentos e eliminar os efeitos de vari√°veis ‚Äã‚Äãque nada t√™m a ver com o tratamento , mas que, no entanto, est√£o influenciando os resultados.

Al√©m disso, permite obter mais informa√ß√µes sobre a natureza dos tratamentos que estamos aplicando em nossa pesquisa. Em resumo, nos ajuda a ajustar nossos resultados para torn√°-los mais confi√°veis.

https://pt.slideshare.net/UbirajaraFernandes/ancova-anlise-de-covarincia-ecologia-quantitativa-ubirajara-l-fernandes

https://maestrovirtuale.com/analise-de-covariancia-ancova-o-que-e-e-como-e-usado-em-estatistica/

---
---
---

## Regress√£o e Regulariza√ß√£o

### O que √© regress√£o? Que modelos voc√™ pode usar para resolver problemas de regress√£o

A regress√£o faz parte da √°rea  de aprendizagem supervisionado de ML. Os modelos de regress√£o investigam a rela√ß√£o entre uma vari√°vel (s) dependente (*target*) independente (s) (*features*).

Exemplos

+ **Regress√£o linear** estabelece uma rela√ß√£o linear entre alvo e preditor (es). Ele prev√™ um valor num√©rico e tem o formato de uma linha **reta**.
+ **Regress√£o polinomial** tem uma equa√ß√£o de regress√£o com o poder da vari√°vel independente maior que 1. √â uma **curva** que se encaixa nos pontos de dados.
+ **Regress√£o de Ridge** ajuda quando preditores s√£o altamente correlacionados (problema de multicolinearidade). Ele penaliza os quadrados dos coeficientes de regress√£o, mas n√£o permite que os coeficientes atinjam zeros (usa a regulariza√ß√£o L2).
+ **Regress√£o do lasso** penaliza os valores absolutos dos coeficientes de regress√£o e permite que alguns deles alcancem o zero absoluto (permitindo a sele√ß√£o de recursos). Usa regulariza√ß√£o L1.
+ **Regress√£o Elastic-Net**: Usa as regulariza√ß√£o L1 e L2.

### Quais m√©tricas para avaliar modelos de regress√£o voc√™ conhece?

<img src="../img/metricas-erros-01.png" />

**Mean Squared Error(MSE) | Erro quadr√°tico m√©dio (EQM)**
+ M√©dia da Somat√≥ria da diferen√ßa entre valor esperado (y) e valor previsto (≈∑) elevado ao quadrado

**Root Mean Squared Error(RMSE) | Raiz do Erro Quadr√°tico M√©dio (REQM**
+ Raiz da MSE

**Mean Absolute Error(MAE) | Erro Absoluto M√©dio (EAM)**
+ Somat√≥rio da diferen√ßa entre o valor esperado (y) e o valor previsto (≈∑) dividido pela quantidade de previs√µes

**R¬≤ or Coefficient of Determination | **Coeficiente de Determina√ß√£o** $ R^2 $. **
+ Basicamente, este coeficiente R¬≤ indica quanto o modelo foi capaz de explicar os dados coletados. O R¬≤ varia entre 0 e 1, indicando, em percentagem, o quanto o modelo consegue explicar os valores observados. Quanto maior o R¬≤, mais explicativo √© o modelo, melhor ele se ajusta √† amostra.
+ Por exemplo, se o R¬≤ de um modelo √© 0,8234, isto significa que 82,34% da vari√°vel dependente consegue ser explicada pelos regressores presentes no modelo.
+ O R¬≤ deve ser usado com precau√ß√£o, pois √© sempre poss√≠vel torn√°-lo maior pela adi√ß√£o de um n√∫mero suficiente de termos ao modelo. Assim, se, por exemplo, n√£o h√° dados repetidos (mais do que um valor `y` para um mesmo `x` ) um polin√¥mio de grau `n - 1` dar√° um ajuste perfeito R¬≤ = 1 para n  dados. Quando h√° valores repetidos, o R¬≤ n√£o ser√° nunca igual a 1, pois o modelo n√£o poder√° explicar a variabilidade devido ao erro puro.
r. 

**Observa√ß√£o: MAE e MSE**
+ MSE e RMSE penalizam outliers e o MAE n√£o
+ Ent√£o, se ao analisar seus dataset, os outliers existirem mas forem realmente parte dos seus dados, ent√£o, √© recomend√°vel usar MSE. 
+ Agora, se puder retirar os outliers, ent√£o √© melhor o MAE

### O que √© Regulariza√ß√£o?

T√©cnica para tratar do problema de overfitting (quando o modelo se adapta demais aos dados de treinamento) ou de underfitting (quando n√£o consegue se ajustar aos dados).

<img src="../img/overffiting-underfiting.png" />

A regulariza√ß√£o coloca mais informa√ß√£o para dar penalidade aos dados que trariam a condi√ß√£o de overfitting/underfitting.

Ele ajuda a reduzir a complexidade do modelo e assim fazer melhores previs√µes.

√â aconselhado em que tem: poucas features para um dataSet muito grande ou ao contr√°rio, quando h√° muitas features para poucos dados.

### Que tipo de t√©cnicas de regulariza√ß√£o s√£o aplic√°veis aos modelos lineares?

Regulariza√ß√£o L1 (regulariza√ß√£o Lasso) - Adiciona a soma dos valores absolutos dos coeficientes √† fun√ß√£o de custo.
Regulariza√ß√£o L2 (regulariza√ß√£o Ridge) - Adiciona a soma dos quadrados dos coeficientes √† fun√ß√£o de custo.

H√° outro que s√£o: AIC/BIC, Ridge, Lasso, Basis pursuit denoising, Rudin‚ÄìOsher‚ÄìFatemi model (TV), Potts model, RLAD, Dantzig Selector,SLOPE

### Podemos usar a regulariza√ß√£o L1 para a sele√ß√£o de features?

Sim, porque a natureza da regulariza√ß√£o L1 (Lasso) levar√° a coeficientes com pouco valor √† zero

Exemplo de Lasso
https://towardsdatascience.com/feature-selection-using-regularisation-a3678b71e499
````python
sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1'))
sel_.fit(scaler.transform(X_train.fillna(0)), y_train)
sel_.get_support()

# sel_.get_support(): Mostrar√° uma matriz de True/False, onde False ser√£o as vari√°veis que foram levadas a Zero

# A seguir, selecionamos as colunas com True
selected_feat = X_train.columns[(sel_.get_support())]
print('total features: {}'.format((X_train.shape[1])))
print('selected features: {}'.format(len(selected_feat)))
print('features with coefficients shrank to zero: {}'.format(
      np.sum(sel_.estimator_.coef_ == 0)))
````

### Quando a regress√£o de Ridge √© favor√°vel em rela√ß√£o √† regress√£o de Lasso?

Na presen√ßa de poucas vari√°veis com um dataset de tamanho m√©dio / grande, use a regress√£o Lasso. 

Na presen√ßa de muitas vari√°veis com efeito de tamanho pequeno / m√©dio, use regress√£o Ridge.

Conceitualmente, podemos dizer que a regress√£o de la√ßo (L1) faz sele√ß√£o de vari√°veis e encolhimento de par√¢metros, enquanto a regress√£o de Ridge apenas encolhe e acaba incluindo todos os coeficientes do modelo. Na presen√ßa de vari√°veis correlacionadas, a regress√£o de Ridge pode ser a escolha preferida. Al√©m disso, a regress√£o de Ridge funciona melhor em situa√ß√µes em que as estimativas menos quadradas t√™m maior varia√ß√£o. Portanto, depende do objetivo do nosso modelo.

---
---
---

## Avaliar Modelos

### Como avaliar modelos de ML, para classifica√ß√£o e regress√£o?

Separa os dados em *train* e *test* de forma aleat√≥ria. Aplica o modelo na base de treinamento e avalia o seu modelo na base de teste.

Pode-se usar a t√©cnica de *cross-validation* para garantir que essa divis√£o √© adequada.

Os crit√©rios de avalia√ß√£o s√£o v√°lidos para determinadas atividade de ML

Classifica√ß√£o (y discreto)
+ Matriz de Confus√£o
+ Curva ROC, CAP e seus respectivos AUC
+ Acur√°cia, Precis√£o, Recall, Sensitividade, Especifidade
+ F1 Score

Regress√£o (y cont√≠nuo)
+ MAE, MSE, RMSE
+ R¬≤

https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b

### Por que precisamos dividir nossos dados em tr√™s partes: treinamento, valida√ß√£o e teste?

O conjunto de treinamento √© usado para ajustar o modelo, ou seja, para treinar o modelo com os dados.

O conjunto de valida√ß√£o √© ent√£o usado para fornecer uma avalia√ß√£o imparcial de um modelo enquanto o ajuste dos hiper par√¢metros √© feito. Isso melhora a generaliza√ß√£o do modelo. 

Finalmente, um conjunto de dados de teste que o modelo nunca ‚Äúviu‚Äù antes deve ser usado para a avalia√ß√£o final do modelo. Isso permite uma avalia√ß√£o imparcial do modelo. A avalia√ß√£o nunca deve ser realizada com os mesmos dados usados para o treinamento. Caso contr√°rio, o desempenho do modelo n√£o seria representativo.



### How do we choose K in K-fold cross-validation? What‚Äôs your favorite K? üë∂

There are two things to consider while deciding K: t
+ he number of models we get and the size of validation set. We do not want the number of models to be too less, like 2 or 3. At least 4 models give a less biased decision on the metrics. On the other hand, we would want the dataset to be at least 20-25% of the entire data. So that at least a ratio of 3:1 between training and validation set is maintained.
I tend to use 4 for small datasets and 5 for large ones as K.

## O que √© a Matriz de confus√£o e tudo o que ele engloba?

**Acur√°cia, Recall, Sensibilidade, F1 Score, Erro tipo 1 e 2**

Exemplo para uma classifica√ß√£o bin√°ria temos a seguinte matriz de confus√£o:

````
Matrix de Confus√£o    Nomenclatura das partes
 [ 65  3  ]               [ TP  FP ]
 [  3  29 ]               [ FN  TN ]
````

Onde:
+ TP (True Positive)  | Verdadeiro Positivo : A Classe √© 1 e Previu 1 (O modelo acertou)
+ FP (False Positive) | Falso Positivo : A Classe √© 1 e Previu 0 (O modelo errou) (erro tipo 1)
+ FN (False Negative) | Falso Negativo : A Classe √© 0 e Previu 1 (O modelo errou) (erro tipo 2)
+ TN (True Negative)  | Verdadeiro Negativo : A Classe √© 0 e Previu 0 (O modelo Acertou)

<img src="../img/metricas-erros-02.png" />

#### Erro Tipo 1 e tipo 2

**Erro tipo 1 / Taxa de Falso Positivo | False Positive Rate**
+ Quando afirma que pertence a uma classe quando na verdade n√£o pertence
+ Erro Tipo 1 = FP/(FP + TN)

**Erro Tipo 2 / Taxa de Falso Negativo | False Negative Rate**
+ Quando afirma que n√£o pertence a uma classe mas na verdade pertence
+ Erro Tipo 2 = FN/(FN + TN)

<div style="text-align: center;">
	<img src="../img/metricas-erros-03.jpg" style="width: 70%; align-content: center" />	
</div>

**Acur√°cia / Accuracy**

M√©trica poss√≠vel para avaliar o modelo de classifica√ß√£o para todas as classes.

Acur√°cia √© a porcentagem total dos itens classificados **corretamente**

<div style="text-align: center;">
	<img src="../img/metricas-erros-03.png" />
</div>

Por√©m, em um dataset desbalanceado ela n√£o ser√° uma m√©trica t√£o boa.

**Observa√ß√£o: Nem sempre acur√°cia √© uma boa m√©trica**

A Acurr√°cia n√£o √© uma boa m√©trica  quando h√° o dataSet est√° desbalanceado na quantidade de registros por classe. Por exemplo, na classifica√ß√£o bin√°ria com 95% da classe A e 5% da classe B, a precis√£o da previs√£o pode ser de 95%. Em datasets desbalanceados, precisamos escolher Precis√£o, Recall ou F1 Score, dependendo do problema que estamos tentando resolver.


<img src="../img/metricas-erros-04.png" />

#### **Precis√£o, Precision**

√â a taxa da quantidade de itens positivos que foram devidamente classificados como positivos, ou seja, a taxa de acerto para classificar os itens de uma classe.

Precis√£o = TP / (TP + FP)
<div style="text-align: center;">
<img src="../img/metricas-erros-06.png" />
</div>

#### Sensitividade, Recall, hit rate TPR (True Positive Rate)

Taxa de itens positivos a uma classe, que fora classificados como positivo pelo modelo do total de itens positivos.

Recall = TP / (TP + FN)

<!--
The recall is alternatively called a true positive rate. It refers to the number of positives that have been claimed by your model compared to the number of positives that are available throughout the data.
-->

<div style="text-align: center;">
	<img src="../img/metricas-erros-05.png" />
</div>

#### Especificidade, Seletividade Specifity, TNR (True Negative Rate)

Taxa de itens previstos como n√£o pertencente a classe do total desses itens negativos a essa classe.

Especificidade = TN/ ( TN + FP)
<div style="text-align: center;">
<img src="../img/metricas-erros-08.png" />
</div>

#### F1 Score

√â a m√©dia harm√¥nica entre precis√£o e recall. √â uma medida melhor que o da acur√°cia quando as classe do dataSet est√£o desbalanceada pois ela vai refletir esse desbalanceamento.

F1 Score = 2 * Precis√£o * Recall / ( Precis√£o + Recall)

A m√©dia harm√¥nica captura quando a quantidade de registros de uma classe √© maior do que outra.

Exemplo:

<div style="text-align: center;">
<img src="../img/metricas-erros-11.png" />
</div>

#### Matrix de confus√£o em scikit-learnig

Exemplo: Para um dataSet com dados de um poss√≠vel cliente para comprar ou n√£o um produto temos

````python
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
precisao = accuracy_score(y_test, y_pred)
matriz = confusion_matrix(y_test, y_pred)
print("Accuracy\n", precisao, "\n")
print("Matrix de Confusao\n",matriz, "\n")
print("Matrix de Confusao Porcentagem\n",matriz/matriz.sum(), "\n")
print(classification_report(y_test,y_pred, target_names=['Not Purchased', 'Purchased']))
````

Gerando

````python
Accuracy
 0.94 

Confusion Matrix
 [[65  3]
 [ 3 29]] 

Confusion Matrix Percentage
 [[0.65 0.03]
 [0.03 0.29]] 

               precision    recall  f1-score   support

Not Purchased       0.96      0.96      0.96        68
    Purchased       0.91      0.91      0.91        32

     accuracy                           0.94       100

Avaliando a classificacao para cada classe:
+ Precision: Numero total de positivos do total dos classificados como positivos para uma classe
+ Recall: Numero de Positivos que foram devidamente identificados para uma classe
+ F1-Score: Media Harmonica entre precisao e recall
+ Support: Quantidade de amostras de uma classe

Avaliacao geral
+ Acurracia
````

### Curva ROC e AUC ROC

A curva ROC representa uma rela√ß√£o entre sensibilidade (RECALL - ) e especificidade (N√ÉO PRECIS√ÉO) e √© comumente usada para medir o desempenho de classificadores bin√°rios.

**Interpret√ß√£o**
+ E quando mais curvado e distante da diagonal , melhor √© o desempenho do seu modelo.
+ Quanto mais pr√≥ximo a curva do seu modelo da diagonal pior ser√° o desempenho do modelo.
**Par√¢metros**

+ TPR (true Positive Rate - Taxa de Verdadeiro Positivo) tamb√©m chamado de **Sensibilidade** [0,1]

+ FPR (false Positve Rate - Taxa de Falso Positivo) que √© calculado como `1 - ` **Especificidade** [0,1]


<img src="../img/img-roc-auc-cap-04.png"  />

#### O que √© ROC AUC, quando usar e como interpretar?

AUC (√°rea debaixo da curva) ou AUC-ROC (Area Under the Receiver Operating Characteristics) √© um valor num√©rico que resume a curva ROC.RC. Varia entre [0,1] quanto mais pr√≥ximo de 1 melhor.

O interessante do AUC √© que a m√©trica √© invariante em escala, uma vez que trabalha com precis√£o das classifica√ß√µes ao inv√©s de seus valores absolutos. Al√©m disso, tamb√©m mede a qualidade das previs√µes do modelo, independentemente do limiar de classifica√ß√£o.





AUC √© o valor da integral da curva ROC. √â um valor num√©rico entre \[0,1\].

√â feito apartir do e TPR e FPR

Quanto maior o valor do AUC melhor ser√° o modelo.



A seguir √° alguns exemplos de gr√°ficos ROC e valores AUC para entender a correla√ß√£o entre eles

<div style="text-align: center;">
<img src="../img/img-roc-auc-cap-03.png" style="width: 60%" />
</div>
Exemplo de v√°rias ROC

<div style="text-align: center;">
<img src="../img/img-roc-auc-cap-07.png"  />
</div>




### Voc√™ pode explicar como a valida√ß√£o cruzada (`cross-validation`) funciona?

`cross-validation`: √â executar um modelo de ML de v√°rias formas diferentes (mudando a ordem de entrada das rows do dataset)

Ele verifica como determinados resultados de an√°lises estat√≠sticas espec√≠ficas ser√£o medidos quando colocados em um conjunto independente de dados.

A valida√ß√£o cruzada √© o processo para separar seu dataset em dois subconjuntos: conjunto de treinamento e valida√ß√£o e avalia seu modelo para escolher os hiper par√¢metros.

Esse dois subconjuntos s√£o escolhidos aleart√≥riamente. Esse processo iterativamente, selecionando diferentes conjuntos de treinamento e valida√ß√£o, a fim de reduzir o vi√©s que voc√™ teria selecionando apenas um conjunto espec√≠fico para treinamento/valida√ß√£o.

Assim, para um mesmo modelo sai diversos modelos com hiper par√¢metros diferentes, tendo assim scores diferentes.

A avalia√ß√£o final do modelo √© feita a apartir da m√©dia dos scores desse modelo.

### O que √© K-fold cross-validation? üë∂

A valida√ß√£o cruzada K-fold √© um m√©todo de valida√ß√£o cruzada em que selecionamos um hiper par√¢metro k. O conjunto de dados agora est√° dividido em k partes. Agora, tomamos a 1¬™ parte como conjunto de valida√ß√£o e o k-1 restante como conjunto de treinamento. Em seguida, tomamos a 2¬™ parte como conjunto de valida√ß√£o e as partes k-1 restantes como conjunto de treinamento. Assim, cada parte √© usada como conjunto de valida√ß√£o uma vez e as partes restantes do k-1 s√£o reunidas e usadas como conjunto de treinamento. N√£o deve ser usado em dados de s√©ries temporais.

Mesmo que se divida a base em treino e teste para cada K. 

**QUAL A GARANTIA DE QUE CADA K TENHA A MESMA QUANTIDADE DE REGISTROS PARA CADA CLASSE**

**Exemplo infeliz:** Imagina que voc√™ divida a base em 50% para uma classifica√ß√£o bin√°ria. Se a os 50% que voc√™ pegar para treinamento s√≥ tiver 1 classe, ent√£o, ele n√£o vai treinar corretamente para avaliar a outra classe.

Ent√£o, temos que garantir que no treinamento haja a mesma propor√ß√£o de amostrar por classe


---
---
---

## Engenharia de Features

### DS-008 - Quais t√©cnicas utilizadas para tratamento de vari√°veis categ√≥ricas?

Label Encoding
+ Usada quando h√° poucos valores √∫nicos categ√≥ricos
+ Mapeia cada valor para um N√∫mero
+ Aten√ß√£o: Use-o para quando o valor categ√≥rico poder ser convertido numa representa√ß√£o num√©rica:
  - Exemplo: Ruim, Bom, √ìtimo [1,2,3]
  - Pois os valores num√©ricos ter√£o impacto na aprendizagem, pois vai considerar o valor 1 mais fraco que 3 (no exemplo acima)

One Hot Encoding
+ Usada quando a vari√°vel categ√≥rica tem diversos valores
+ Para cada valor √∫nico, cria-se uma coluna a mais. √â colocado 0 ou 1 para o caso de ter aquele atributo.
+ Exemplo Um atributo 'cidade' de um estado
  - Se existir X cidades, ent√£o s√£o criadas mais X features, para cada row do dataSet, somente uma dessa X novas features ter√° o valor 1, as outras X-1 features ter√£o valor 0.

### Why do we need one-hot encoding? ‚Äç‚≠êÔ∏è

If we simply encode categorical variables with a Label encoder, they become ordinal which can lead to undesirable consequences. In this case, linear models will treat category with id 4 as twice better than a category with id 2. One-hot encoding allows us to represent a categorical variable in a numerical vector space which ensures that vectors of each category have equal distances between each other. This approach is not suited for all situations, because by using it with categorical variables of high cardinality (e.g. customer id) we will encounter problems that come into play because of the curse of dimensionality.

### DS-001 - Como selecionar as features mais importantes de um DataSet?

+ Remove as features que est√£o correlacionadas

Answer: Following are the methods of variable selection you can use:

Remove the correlated variables prior to selecting important variables
Use linear regression and select variables based on p values
Use Forward Selection, Backward Selection, Stepwise Selection
Use Random Forest, Xgboost and plot variable importance chart
Use Lasso Regression
Measure information gain for the available set of features and select top n features accordingly.

### Q38. When does regularization becomes necessary in Machine Learning?

https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/

Answer: Regularization becomes necessary when the model begins to ovefit / underfit. This technique introduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term. This helps to reduce model complexity so that the model can become better at predicting (generalizing).



### DS-001 - O que fazer com dados corrompidos ou faltantes?

Podemos:
+ Retirar as rows (se forem poucas estiverem corrompidas)
+ Retirar as colunas (se muitas rows estiverem corrompidas)
+ Colocar a m√©dia ou algum valor que fa√ßa sentido.

Dissecar:
https://analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/

---
---
---

## Gradient boosting

### What is gradient boosting trees? ‚Äç‚≠êÔ∏è

Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.


### What‚Äôs the difference between random forest and gradient boosting? ‚Äç‚≠êÔ∏è

Random Forests builds each tree independently while Gradient Boosting builds one tree at a time.
Random Forests combine results at the end of the process (by averaging or ‚Äúmajority rules‚Äù) while Gradient Boosting combines results along the way.

### Q21. Both being tree based algorithm, how is random forest different from Gradient boosting algorithm (GBM)?

https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/

Answer: The fundamental difference is, random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions.

In bagging technique, a data set is divided into n samples using randomized sampling. Then, using a single learning algorithm a model is build on all samples. Later, the resultant predictions are combined using voting or averaging. Bagging is done is parallel. In boosting, after the first round of predictions, the algorithm weighs misclassified predictions higher, such that they can be corrected in the succeeding round. This sequential process of giving higher weights to misclassified predictions continue until a stopping criterion is reached.

Random forest improves model accuracy by reducing variance (mainly). The trees grown are uncorrelated to maximize the decrease in variance. On the other hand, GBM improves accuracy my reducing both bias and variance in a model.

---
---
---

## Parameter tuning


### Which hyper-parameter tuning strategies (in general) do you know? ‚Äç‚≠êÔ∏è

There are several strategies for hyper-tuning but I would argue that the three most popular nowadays are the following:

Grid Search is an exhaustive approach such that for each hyper-parameter, the user needs to manually give a list of values for the algorithm to try. After these values are selected, grid search then evaluates the algorithm using each and every combination of hyper-parameters and returns the combination that gives the optimal result (i.e. lowest MAE). Because grid search evaluates the given algorithm using all combinations, it‚Äôs easy to see that this can be quite computationally expensive and can lead to sub-optimal results specifically since the user needs to specify specific values for these hyper-parameters, which is prone for error and requires domain knowledge.

Random Search is similar to grid search but differs in the sense that rather than specifying which values to try for each hyper-parameter, an upper and lower bound of values for each hyper-parameter is given instead. With uniform probability, random values within these bounds are then chosen and similarly, the best combination is returned to the user. Although this seems less intuitive, no domain knowledge is necessary and theoretically much more of the parameter space can be explored.

---
---
---

## Dimensionality reduction

### What is the curse of dimensionality? Why do we care about it? ‚Äç‚≠êÔ∏è

Data in only one dimension is relatively tightly packed. Adding a dimension stretches the points across that dimension, pushing them further apart. Additional dimensions spread the data even further making high dimensional data extremely sparse. We care about it, because it is difficult to use machine learning in sparse spaces.



### Do you know any dimensionality reduction techniques? ‚Äç‚≠êÔ∏è

Singular Value Decomposition (SVD)
Principal Component Analysis (PCA)
Linear Discriminant Analysis (LDA)
T-distributed Stochastic Neighbor Embedding (t-SNE)
Autoencoders
Fourier and Wavelet Transforms


### What‚Äôs singular value decomposition? How is it typically used for machine learning? ‚Äç‚≠êÔ∏è

Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Œ£ (diagonal matrix) and R^T (right singular values).
For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive.
Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction.

## O que √© PCA?

PCA Intuition
What is the true purpose of PCA?
The true purpose is mainly to decrease the complexity of the model. It is to simplify the model while keeping relevance and performance. Sometimes you can have datasets with hundreds of features so in that case you just want to extract much fewer independent variables that explain the most the variance.

What is the difference between PCA and Factor Analysis?
Principal component analysis involves extracting linear composites of observed variables. Factor analysis is based on a formal model predicting observed variables from theoretical latent factors. PCA is meant to maximize the total variance to look for distinguishable patterns, and Factor analysis looks to maximize the shared variance for latent constructs or variables.

Should I apply PCA if my dataset has categorical variables?
You could try PCA, but I would be really careful, because categorical values can have high variances by default and will usually be unstable to matrix inversion.

Apply PCA and do cross validation to see if it can generalize better than the actual data. If it does, then PCA is good for your model. (Your training matrix is numerically stable). However, I am certain that in most cases, PCA does not work well in datasets that only contain categorical data. Vanilla PCA is designed based on capturing the covariance in continuous variables. There are other data reduction methods you can try to compress the data like multiple correspondence analysis and categorical PCA etc.

What is the best extra resource on PCA?
Check out this video that has an amazing explanation of PCA and studies it in more depth.

---

Significa: "An√°lise de Componetnes principais"

=> Para problemas linearmente separ√°veis

=> Deve-se ser feito asobre dados num√©ricos

**Diferenciar Sele√ß√¢o de Extra√ß√£o de caracter√≠sticas**

‚Ä¢ Sele√ß√£o de caracter√≠sticas x Extra√ß√£o de caracter√≠sticas
+ Sele√ß√£o: Indicar os atributos mais importantes
+ Extra√ß√£o: AO fazer uma an√°lise, criar novos atributos. √â como unir atributos.
  - Encontrar relacionamentos entre os atributos para combinar eles e reduzir a dimensionalidade

Lembre-se, o PCA n√¢o √© escolher os melhore "n" atributos, e sim reduzir para "n" atributos

**Caracter√≠sticas do PCA**

‚Ä¢ PCA: Identifica a correla√ß√£o entre vari√°veis, e caso haja uma forte
correla√ß√£o √© poss√≠vel reduzir a dimensionalidade.
  - Exemplo: Se voc√ä t√™m duas vari√°veis com forte correla√ß√£o voc√ä pde ent√£o unilas e asism reduzir em 1 a quantidade de features.

**Funcionamento**

‚Ä¢ Um dos principais algoritmos de aprendizagem de m√°quina n√£o
supervisionada (n√£o h√° certo/errado apriori)
‚Ä¢ Das m vari√°veis independentes, PCA extrai p <= m novas vari√°veis
independentes que explica melhor a varia√ß√£o na base de dados, sem
considerar a vari√°vel dependente
‚Ä¢ O usu√°rio pode escolher o n√∫mero de p

COMPONETSE PRICINCPAIS S√ÇO OS COMEPONETNE DE features concatenados.

**Se com PCA fiacr pior?**

Abaixou um pouco, mas, voc√™ deve avaliar o `trade_off` entre a precisao e a velocidade.

Exemplo, ser√° que mesmo reduzindo 1\% poderia ser melhor usar o PCA pois teria menos dados para classificar e assim ter menos custo computacional?

````python

pca.explained_variance_ratio_ Revela o quanto os componentes definem os dados, a import√¢ncia deles.

Inicialmente, deve-se colocar PCA(n_components = None) para descobrir a variance_ratio para cada feature. Depois disos, escolher um n√∫mero menor que len(features) cuja soma seja alta.

Um exemplo, pr√°tico. Calcular o PCA para 30 feautures e descobrir que, 3 features tem a soma de variance_ratio de 0.80. Ou seja, somente 3 vari√°veis j√° seriam necess√°rio para contextualizar sua base com 80% de veracidade.

componentes = pca.explained_variance_ratio_
componentes
executed in 17ms, finished 16:07:54 2020-01-07
array([0.151561  , 0.10109701, 0.08980379, 0.08076277, 0.07627678,
       0.07357646])
componentes.sum()
executed in 31ms, finished 16:42:58 2020-01-07
0.5730778058904467

````
## LDA

**Questions LDA - English ML-AZ**
LDA Intuition
Could you please explain in a more simpler way the difference between PCA and LDA?
A simple way of viewing the difference between PCA and LDA is that PCA treats the entire data set as a whole while LDA attempts to model the differences between classes within the data. Also, PCA extracts some components that explain the most the variance, while LDA extracts some components that maximize class separability.

Feature Selection or Feature Extraction?
You would rather choose feature selection if you want to keep all the interpretation of your problem, your dataset and your model results. But if you don‚Äôt care about the interpretation and only car about getting accurate predictions, then you can try both, separately or together, and compare the performance results. So yes feature selection and feature extraction can be applied simultaneously in a given problem.

Can we use LDA for Regression?
LDA is Linear Discriminant Analysis. It is a generalization of Fisher‚Äôs linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification. However, for regression, we have to use ANOVA, a variation of LDA. LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made. LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.

LDA in Python
Which independent variables are found after applying LDA?
The two independent variables that you see, indexed by 0 and 1, are new independent variables that are not among your 12 original independent variables. These are totally new independent variables that were extracted through LDA, and that‚Äôs why we call LDA Feature Extraction, as opposed to Feature Selection where you keep some of your original independent variables.

How to decide the LDA n_component parameter in order to find the most accurate result?
You can run:

LDA(n\_components = None)
and it should give you automatically the ideal n_components.

How can I get the two Linear Discriminants LD1 and LD2 in Python?
You can get them by running the following line of code:

lda.scalings_

---

**LDA (Linear Discriminant Analysis)**

Possui mesma fun√ß√£o do PCA mas envolve a classe dos dados, ou seja, √© um algortimo supervisionado.

=> Para problemas linearmente separ√°veis

‚Ä¢ Al√©m de encontrar os componentes principais, LDA tamb√©m encontra os eixos que maximizam a separa√ß√£o entre m√∫ltiplas classes

‚Ä¢ √â um algoritmo supervisionado por causa da rela√ß√£o que tem com as classes

‚Ä¢ Das m vari√°veis independentes, LDA extrai p <= m novas vari√°veis independentes que mais separam as classes da vari√°vel dependente


---
---
---

## Case Studies

### Q4. You are given a data set on cancer detection. You‚Äôve build a classification model and achieved an accuracy of 96%. Why shouldn‚Äôt you be happy with your model performance? What can you do about it?

https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/

Answer: If you have worked on enough data sets, you should deduce that cancer detection results in imbalanced data. In an imbalanced data set, accuracy should not be used as a measure of performance because 96% (as given) might only be predicting majority class correctly, but our class of interest is minority class (4%) which is the people who actually got diagnosed with cancer. Hence, in order to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine class wise performance of the classifier. If the minority class performance is found to to be poor, we can undertake the following steps:

We can use undersampling, oversampling or SMOTE to make the data balanced.
We can alter the prediction threshold value by doing probability caliberation and finding a optimal threshold using AUC-ROC curve.
We can assign weight to classes such that the minority classes gets larger weight.
We can also use anomaly detection.
Know more: Imbalanced Classification
